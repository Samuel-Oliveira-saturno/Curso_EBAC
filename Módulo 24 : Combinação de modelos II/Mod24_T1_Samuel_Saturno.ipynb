{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/media/logo/newebac_logo_black_half.png\" alt=\"ebac-logo\">\n",
        "\n",
        "---\n",
        "\n",
        "# **Módulo 24** | Combinação de modelos II | Boosting vs AdaBoost\n",
        "Caderno de **exercício 01**<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Discente: Samuel Saturno\n"
      ],
      "metadata": {
        "id": "IQZl-knkK6k0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><font  color=\"bronw\"><strong>1. Cite 5 diferenças entre o Random Forest e o\n",
        "AdaBoost</strong></font></h3>\n",
        "\n",
        "<h4><font  color=\"\"><strong>Passo 1: Entendimento do Bagging</strong></font></h4>\n",
        "\n",
        "**1. Algoritmo Base:**\n",
        "\n",
        "<h4><font  color=\"\"><i>\n",
        "\n",
        "* RandomForest: Usa um conjunto de árvores de decisão como base para criar um modelo robusto.\n",
        "* AdaBoost: Usa um único tipo de classificador fraco (por exemplo, árvore de decisão) como base, mas itera sobre ele para melhorar o desempenho do modelo combinando várias versões do classificador.\n",
        "\n",
        "</i></font></h4>\n",
        "\n",
        "**2. Ponderação das Instâncias:**\n",
        "\n",
        "<h4><font  color=\"\"><i>\n",
        "\n",
        "* RandomForest: Cada árvore no ensemble é treinada independentemente com uma amostra aleatória do conjunto de dados, e as instâncias têm pesos iguais.\n",
        "* AdaBoost: Dá mais peso às instâncias mal classificadas em cada iteração subsequente, focando em corrigir as previsões erradas das iterações anteriores.\n",
        "\n",
        "</i></font></h4>\n",
        "\n",
        "**3. Ponto Fraco:**\n",
        "\n",
        "<h4><font  color=\"\"><i>\n",
        "\n",
        "* RandomForest: Tende a não funcionar bem em dados desbalanceados ou com ruído, pois cada árvore é construída de forma independente.\n",
        "* AdaBoost: É sensível a outliers e ruído nos dados, pois tenta corrigir erros anteriores, o que pode aumentar a sensibilidade a instâncias mal rotuladas.\n",
        "\n",
        "</i></font></h4>\n",
        "\n",
        "**4. Processo de Treinamento:**\n",
        "\n",
        "<h4><font  color=\"\"><i>\n",
        "\n",
        "* RandomForest: Treina várias árvores de decisão de forma paralela, com amostragem aleatória de dados e seleção aleatória de características em cada árvore.\n",
        "* AdaBoost: Treina iterativamente um classificador fraco em cada iteração, dando mais peso às instâncias classificadas incorretamente nas iterações anteriores.\n",
        "\n",
        "</i></font></h4>\n",
        "\n",
        "**5. Robustez e Interpretabilidade:**\n",
        "\n",
        "<h4><font  color=\"\"><i>\n",
        "\n",
        "* RandomForest: É mais robusto em relação a overfitting devido à combinação de várias árvores, mas pode ser menos interpretável devido à complexidade do modelo.\n",
        "* AdaBoost: Pode sofrer mais com overfitting se o classificador fraco não for bem escolhido, mas geralmente é mais fácil de interpretar, especialmente com classificadores fracos simples como árvores de decisão.\n",
        "\n",
        "</i></font></h4>\n",
        "\n",
        "\n",
        "\n",
        "<h5><font  color=\"\"><strong>Referências Bibliográticas (visto em 19/04/2024)</strong></font></h5>\n",
        "\n",
        "<h6><font  color=\"blue\"><strong>Sites:</strong></font></h6>\n",
        "\n",
        "https://scikit-learn.org/stable/modules/ensemble.html\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sfee7KzBDj_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<h3><font  color=\"Bronw\"><strong> 2. Acesse o link Scikit-learn – adaboost, leia a\n",
        "explicação (traduza se for preciso) e crie um\n",
        "jupyter notebook contendo o exemplo do\n",
        "AdaBoost. </strong></font></h3>\n",
        "\n",
        "<h3><font  color=\"blue\"><strong> 1.11.7.1. Usage </strong></font></h3>\n",
        "\n",
        "\n",
        "<h5><font  color=\"\"><strong>\n",
        "\n",
        "O exemplo a seguir mostra como ajustar um classificador AdaBoost com 100 classificadores fracos:\n",
        "\n",
        "</strong></font></h5>\n"
      ],
      "metadata": {
        "id": "CBMU9pqA9mYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa a função cross_val_score para realizar validação cruzada\n",
        "# Importa a função load_iris para carregar o conjunto de dados Iris\n",
        "# Importa a classe AdaBoostClassifier para criar um classificador AdaBoost\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Carrega os dados do conjunto de dados Iris e atribui a X os dados de entrada e a y os rótulos de saída\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Cria um classificador AdaBoost com 100 estimadores (árvores de decisão por padrão) e o algoritmo \"SAMME\"\n",
        "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\")\n",
        "\n",
        "# Realiza a validação cruzada com 5 dobras (cv=5) no classificador clf usando os dados X e y\n",
        "scores = cross_val_score(clf, X, y, cv=5)\n",
        "\n",
        "# Calcula a média das pontuações de validação cruzada para obter uma estimativa do desempenho do modelo\n",
        "scores.mean()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBna4zSz9Rvs",
        "outputId": "cfd3e328-dede-4269-a8e5-c4ad0c972bde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9533333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><font  color=\"bronw\"><strong>3. Cite 5 Hyperparametros importantes no AdaBoost.</strong></font></h3>\n",
        "\n",
        "<h4><font  color=\"\"><strong>Passo 1: Entendimento do Bagging</strong></font></h4>\n",
        "\n",
        "**1. n_estimators:**\n",
        "\n",
        "<h4><font  color=\"\"><i>\n",
        "\n",
        "* Este parâmetro especifica o número de aprendizes fracos (por exemplo, árvores de decisão) que serão combinados para formar o modelo AdaBoost.\n",
        "\n",
        "</i></font></h4>\n",
        "\n",
        "**2. learning_rate:**\n",
        "\n",
        "<h4><font  color=\"\"><i>\n",
        "\n",
        "* Controla a contribuição de cada aprendiz fraco para a atualização do modelo. Um valor menor reduz a contribuição de cada aprendiz, o que pode melhorar a generalização do modelo.\n",
        "\n",
        "</i></font></h4>\n",
        "\n",
        "**3. base_estimator:**\n",
        "\n",
        "<h4><font  color=\"\"><i>\n",
        "\n",
        "* Especifica o tipo de aprendiz fraco a ser utilizado no processo de treinamento do AdaBoost.\n",
        "\n",
        "</i></font></h4>\n",
        "\n",
        "**4. Algorithm:**\n",
        "\n",
        "<h4><font  color=\"\"><i>\n",
        "\n",
        "* Determina como os pesos das amostras são atualizados em cada iteração do treinamento.\n",
        "\n",
        "</i></font></h4>\n",
        "\n",
        "**5. random_state:**\n",
        "\n",
        "<h4><font  color=\"\"><i>\n",
        "\n",
        "* Controla a aleatoriedade no processo de treinamento, garantindo reprodutibilidade dos resultados.\n",
        "\n",
        "</i></font></h4>\n",
        "\n",
        "\n",
        "\n",
        "<h5><font  color=\"\"><strong>Referências Bibliográticas (visto em 19/04/2024)</strong></font></h5>\n",
        "\n",
        "<h6><font  color=\"blue\"><strong>Sites:</strong></font></h6>\n",
        "\n",
        "https://scikit-learn.org/stable/modules/ensemble.html"
      ],
      "metadata": {
        "id": "KtjjjkZvBFOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<h3><font  color=\"bronw\"><strong>\n",
        "5.(Opcional) Utilize o GridSearch para encontrar\n",
        "os melhores hyperparametros para o conjunto\n",
        "de dados do exemplo (load_iris)\n",
        "</strong></font></h3>"
      ],
      "metadata": {
        "id": "OI4kNbfQCxqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Defina a grade de hiperparâmetros a serem testados\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Teste diferentes números de estimadores\n",
        "    'learning_rate': [0.01, 0.1, 1.0]  # Teste diferentes taxas de aprendizado\n",
        "}\n",
        "\n",
        "# Instancie o classificador AdaBoost\n",
        "clf = AdaBoostClassifier(algorithm=\"SAMME\")\n",
        "\n",
        "# Instancie o GridSearchCV com o classificador e a grade de hiperparâmetros\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "\n",
        "# Ajuste o GridSearch aos dados\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Exiba os melhores hiperparâmetros encontrados\n",
        "print(\"Melhores hiperparâmetros encontrados:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Exiba a melhor pontuação de validação cruzada\n",
        "print(\"Melhor pontuação de validação cruzada:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFSzC6mr9TNF",
        "outputId": "4453faa4-faf7-4bd7-f54f-ba34a2e8035d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores hiperparâmetros encontrados:\n",
            "{'learning_rate': 0.1, 'n_estimators': 200}\n",
            "Melhor pontuação de validação cruzada: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**\n",
        "<h4><font color=\"\"><i>\n",
        "Os resultados do GridSearch revelam que os melhores hiperparâmetros encontrados para o classificador AdaBoost são uma taxa de aprendizado de 0.1 e 200 estimadores (árvores de decisão, por padrão). Além disso, a melhor pontuação de validação cruzada obtida foi de 0.96, o que indica um desempenho bastante sólido do modelo em dados não vistos. Isso sugere que o modelo é capaz de generalizar bem para novos dados, com uma taxa de precisão de aproximadamente 96%. Esses resultados podem ser úteis para otimizar ainda mais o modelo e melhorar seu desempenho em aplicações práticas.\n",
        "</i></font></h4>"
      ],
      "metadata": {
        "id": "lroArwXgDs6L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wr6G25PEEyJl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}